{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from nltk import ngrams\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation, RepeatVector,Flatten, TimeDistributed, Input,Bidirectional,LocallyConnected1D,Conv1D,GlobalAveragePooling1D,GlobalMaxPooling1D,Concatenate,BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, LSTM ,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers\n",
    "# from tensorflow.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# import tensorflow.keras.utils.to_categorical as to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "For this task we will be using the many to many archetecture(the first one from the left)\n",
    "<img src=\"archetecturernn.png\">\n",
    "This archtecture is called the encoder-decoder or the sequence 2 sequence(Seq2seq) model, this is usually used when there is a mismatch between the input and output sequence lengths but for your learning we will use this nonetheless. For this task we will be using the brown [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus), you can import this from nltk.corpus.Here is a visual guide for how this will work:\n",
    "![alt text](https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png)\n",
    "This image ilustrates the task of translation from one language to another. We will instead use the same technique for POS tagging.<br>\n",
    "<b>How the mode works:</b> The model uses 2 LSTM's. The first LSTM encodes the sentence into feature vector of sorts and the second LSTM then uses this vector to predict a sequence of POS tags. After each prediction we will provide the actual tag instead of the predicted so that our model learns faster this technique is called <a href=\"https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c\">Teacher Forcing</a><br>\n",
    "<h3>Data Preperation</h3>\n",
    "<ul>\n",
    "    <li> First we need to preprocess the data, convert the data to lower casing.</li>\n",
    "    <li> Split the data into test, train and validation in the ratio 20,70,10. Use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">scikit_test_train_split</a> <br><i><b>Hint:</b> use the splitter twice to get desired data splits.</i></li>\n",
    "    <li> For this task our model will have 2 inputs the sentence and the actual tag sequence. The labels/targets will also be the actual tag sequence.Inputs=(sentence,input tag sequence), Output=(output tag sequence)</li>\n",
    "    <li> For the input tag sequence append start of sentence '[START]'or '[SOS] tag at start of the sentence and for prediction tag sequence append  end of sentence '[STOP]' or '[EOS]' tag. These tag help the model learn when to start and end a sequence of predictions</li>\n",
    "    <li> Repeat the process of extracting vocabulary and numeric encoding for both the inputs and one hot encodings for prediction tag sequence.<li>\n",
    "</ul>\n",
    "<h3>Embeddings</h3>\n",
    "<ul>\n",
    "    <li>For this part instead of using pretrained embeddings you will use train them from scratch</li>\n",
    "</ul>\n",
    "<h3>Create Model</h3>\n",
    "<ul>\n",
    "    <li> Create the model using <a href\"https://www.tensorflow.org/guide/keras/functional\">functional API</a></li>\n",
    "    <li> Hints: The emebedding layer has a parameter that allows you to use pretrained embeddings, for shared layers read the section of shared layer weights in function API docs</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
