{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 CS 5316 Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from nltk import ngrams\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation, RepeatVector,Flatten, TimeDistributed, Input,Bidirectional,LocallyConnected1D,Conv1D,GlobalAveragePooling1D,GlobalMaxPooling1D,Concatenate,BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, LSTM ,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers\n",
    "# from tensorflow.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# import tensorflow.keras.utils.to_categorical as to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assingmnet\n",
    "This is going to be the final assignment for deep learning. Here is a very good visual for what you will be doing with\n",
    "<a href=\"https://ibb.co/mh9Ks0j\">deep learning.</a> Lets get started......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1 Paraphrase Detection\n",
    "For this task we will be using the [ Microsoft Research Paraphrase Corpus ](https://www.microsoft.com/en-us/download/details.aspx?id=52398). The corpus consist of sentence pairs with 1 or 0 labels which identify if the sentences are paraphrase or not respectively.\n",
    "<br>\n",
    "To perform this task we will be using recurrenct neural network for this task specifically the [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). RNN can be architected in multiple ways. Some of the possible ways are as follows:\n",
    "<img src=\"archetecturernn.png\">\n",
    "The box in the bottom is the input, followed by the hidden layer (as the middle box), and the box on top is the output layer. The one-to-one architecture is the typical neural network (<i>vanila/Feed Forward</i>) with a hidden layer between the input and the output layer. Example uses of the above archetecture are as follows:\n",
    "<ul>\n",
    "    <li>One-to-many: input is an image and outputs are image captions</li>\n",
    "    <li>Many-to-one: input is a movie's review <i>multiple words in input</i> and output is sentiment associated with the review <i>(we will be using a similar archetecture for our purpose)</i></li>\n",
    "    <li>Many-to-many: machine translation of a sentence in one language to a sentence in another language, POS tagging etc</li>\n",
    "</ul>\n",
    "<br>\n",
    "For this task we will also be using pre-trained word embeddings specificallly <a href=\"https://nlp.stanford.edu/projects/glove/\">(GloVe Embeddings)</a>. Please download the paraphrase <a href=\"https://www.microsoft.com/en-us/download/details.aspx?id=52398\">dataset</a> and glove.6B.zip from <a href=\"https://nlp.stanford.edu/projects/glove/\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task you are required to implement the following archetecture, please use [keras functional API](https://www.tensorflow.org/guide/keras/functional) :\n",
    "<img src=\"paraphrase.png\">\n",
    "If <a href=\"https://ibb.co/RSSjRM0\">this</a> is you reaction after seeing the model archetecture dont worry we'll explain.\n",
    "The model works as follows, there will be two inputs layers, one for each sentence followed by <b>shared</b> embedding layer which feed thier outputs to the shared LSTM, <b>take the final hidden state output</b> of both LSTM's and concatenate them. Finally feed the concatenated vector to a softmax output layer for classification.\n",
    "<br>\n",
    "<i>(The reason for using one shared embedding and LSTM layer so that the model learns sentence representation for all sentence pairs(x,x') in the dataset. If we were using two seperate LSTMS for x and x' we would need to double the dataset by having both (x,x') and (x',x) pairs so that both LSTM's see the entire train data distribution)</i>\n",
    "The purpose for each layer in the model is as follows:\n",
    "<ul>\n",
    "    <li>Input takes the input sequences and feeds it to the next(you will need to specify the maximum size of a sentence as a parameter of this layer)</li>\n",
    "    <li>Embedding layer, this layer takes the sequence input then for each word in the sequence generates a fixed size vector <i>(word embedding)</i>, this layer can be trained from scratch or can be configured to use pretrained embeddings with or without fine tuning. </li>\n",
    "    <li>LSTM process the embedding vector sequences and at each step generates a hidden state vector(h)and cell memory vector(C)(<i>see diagram</i>), the keras LSTM layer returns three outputs (1) All the hidden states,(2) The final hidden state and (3) The final cell memory state<img src=\"lstm.png\"></li>\n",
    "    <li>The concatenation layer combines multple vectors into a single vector</li>\n",
    "    <li>Finally the output layer predicts if sentence pairs were paraphrase or not</li>\n",
    "</ul>\n",
    "<b>Please refer to the TF-keras documentation for all the layers <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers\">here</a></b>\n",
    "<br><br><br>\n",
    "Now that you understand the theoritical foundation for our approach lets move onto practical implementation.<br>\n",
    "<h3>Data Preperation</h3>\n",
    "\n",
    "<ul>\n",
    "    <li> First we need to preprocess the data, convert the data to lower casing. Any other preprocessing procedures are optional but keep in mind that this will affect the performance of your model.</li>\n",
    "    <li> To make training faster we will fix the maximum sequence length to 20 truncate the longer sequences.</li>\n",
    "    <li> Split the data into test, train and validation in the ratio 20,70,10. Use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">scikit_test_train_split</a> <br><i><b>Hint:</b> use the splitter twice to get desired data splits.</i></li>\n",
    "    <li> Next we need the vocabulary, vocabulary size and to convert sentences to numeric sequences by representing each word with a numeric value which will make our implementation easier later on, use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\">Tokenizer</a> from keras. <br><i>(Fit the tokenizer on train data and use the same tokenizer to convert train,test and validation data to numeric sequences)</i> </li>\n",
    "    <li>  Use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\"> pad sequences</a> to add post padding to all sentences that are shorter than maximum sequence length\n",
    "        <i>(<b>extra info</b>: fit_on_text reserves value/index 0 for padding and assigns numeric value to words starting from index 1)</i></li>\n",
    "</ul>\n",
    "<h3>Loading embeddings</h3>\n",
    "<ul>\n",
    "    <li> To use pretrained embeddings in tf keras embedding layer requires a dictionary, we need to create a dictionary whose keys will be numeric word representations and values will be the embedding vectors.</li>\n",
    "    <li> First step is to load the word embedding pairs from the glove file into a dictionary.</li>\n",
    "    <li> Next we will create a dictionary for our dataset's vocabulary. Copy all the word embeddings for words that are in our vocabulary and in the glove dictionary, if a word exists in our vocabulary but does not exist in glove dictionary create a zero vector of embedding dimension size and add it to the dictionary.</li>\n",
    "</ul>\n",
    "<h3>Create Model</h3>\n",
    "<ul>\n",
    "    <li> Create the model using <a href\"https://www.tensorflow.org/guide/keras/functional\">functional API</a></li>\n",
    "    <li> Hints: The emebedding layer has a parameter that allows you to use pretrained embeddings, for shared layers read the section of shared layer weights in function API docs</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    \"\"\"\n",
    "    Return preprocessed data\n",
    "    \n",
    "    Returns: X and Y where X is pair of sentence (x,x') and y is the label 0 or 1\n",
    "    \"\"\"\n",
    "#     data_X=\n",
    "#     data_Y=\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    \"\"\"\n",
    "    Return preprocessed data\n",
    "\n",
    "    Args:\n",
    "        data : sentence pairs\n",
    "    \n",
    "    Returns: preprocessed_data\n",
    "    preprocessed_data : preprocessed dataset \n",
    "    \"\"\"\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test train split\n",
    "Use test train split from sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTrainSplit(data_X,data_Y):\n",
    "    \"\"\"\n",
    "    Return test train data\n",
    "\n",
    "    Args:\n",
    "        data_X : sentence pairs\n",
    "        data_Y: labels\n",
    "        \n",
    "    Returns: test train and validation split data \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the step regarding keras Tokenizer in the cell below.<br>\n",
    "<i> Keep in mind that each example is a pair/tupple of sentence(x,x'), combine them into a single sentence so that your data is a list of sentences before calling fit on text(Tokenizer). There is out of vocabulary option in tokenizer check that out aswell.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabular, vocabulary size and numeric word seqeunces for train,test and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model in the cell below:\n",
    "Try out different sizes for LSTM 50,100,300 and use relu activations. Also report results with Bi-LSTM as well.<br>\n",
    "<i>To boost performance you can try adding a hidden layer between the lstm and output layer and also by adding a dropout layer in between different layers</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "filepath = \"setting_\" + \"model1\" + \".hdf5\"\n",
    "logfilepath = \"setting_\"+\"model1\" + \".csv\"\n",
    "reduce_lr_rate=0.2\n",
    "logCallback = CSVLogger(logfilepath, separator=',', append=False)\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', save_weights_only=True, verbose=1,\n",
    "                             save_best_only=True, mode='auto')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=reduce_lr_rate, patience=10,\n",
    "                              cooldown=0, min_lr=0.0000000001, verbose=0)\n",
    "\n",
    "callbacks_list = [logCallback, earlyStopping, reduce_lr, checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_X,train_Y, epochs=100, batch_size=32,\n",
    "#                verbose=1,shuffle=True,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the <b>model.predict</b> method to get predictions. There predictions will be a probability distribution over the lables, to get the desired class take the max value in a prediction vector as the predicted class.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = code here\n",
    "#labelList=[0,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_Y_max=np.argmax(test_Y, axis=-1)\n",
    "cm=confusion_matrix(test_Y_max,predictions)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm = pd.DataFrame(cm, labelList,labelList )# matrix,names row,names col,\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 11}, fmt=\".2f\") # font size\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report\\n\",classification_report(test_Y_max, predictions, labels=[0,1], target_names = labelList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Sentiment Classification\n",
    "For this task we will be reusing the movie reviews dataset available on <a href=\"https://www.kaggle.com/c/word2vec-nlp-tutorial/data\">kaggle</a> and download the dataset from there. \n",
    "We will be using the unlabeledTrainData file and labeledTrainData file. We will use the gensim package to train word2vec embeddings using [gensim](https://radimrehurek.com/gensim/) package and unlabelled train data as in the previous assignmnet. Now instead for creating a single representation for each review we will be using deep learning models for this task. We will use the same archetecture as before but will experiment with different reccurant networks namely RNN, GRU and LSTM.<br> This task might feel like <a href=\"https://ibb.co/Tgh2XyH\">this</a> but since this is a deep learning assignment thus we must use it.\n",
    "<h3>Data Preperation</h3>\n",
    "<ul>\n",
    "    <li> First we need to preprocess the data, convert the data to lower casing(both files). Any other preprocessing procedures are optional but keep in mind that this will affect the performance of your model.</li>\n",
    "    <li> Split the labeledTrainData data file into test, train and validation in the ratio 20,70,10. Use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">scikit_test_train_split</a> <br><i><b>Hint:</b> use the splitter twice to get desired data splits.</i></li>\n",
    "    <li> Next we need the vocabulary, vocabulary size and to convert sentences to numeric sequences by representing each word with a numeric value which will make our implementation easier later on, use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\">Tokenizer</a> from keras. <br><i>(Fit the tokenizer on train data and use the same tokenizer to convert train,test and validation data to numeric sequences)</i> </li>\n",
    "    <li>  Use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\"> pad sequences</a> to add post padding to all sentences that are shorter than maximum sequence length</li>\n",
    "    <li> Use one hot representation for targets/labels, you can use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">scikit learn</a> or <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing\">keras preprocessing</a>.</li>\n",
    "</ul>\n",
    "<h3>Loading embeddings</h3>\n",
    "<ul>\n",
    "    <li> As state before use the gensim package to train the word2vec model on unlabelledTrainData file</li>\n",
    "    <li> Next we will create a dictionary for our dataset's vocabulary. Copy all the word embeddings for words that are in our vocabulary and in the word2vec model, if a word exists in our vocabulary but does not exist in word2vec model create a zero vector of embedding dimension size and add it to the dictionary.</li>\n",
    "</ul>\n",
    "<h3>Create Model</h3>\n",
    "<ul>\n",
    "    <li> Here is a visual for the model <img src=\"sentimentdeep.png\">\n",
    "    <li> Create the model using <a href\"https://www.tensorflow.org/guide/keras/functional\">functional API</a> or the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\">Sequential API</a></li>\n",
    "    <li> Hints: The emebedding layer has a parameter that allows you to use pretrained embeddings</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use can reuse the code snippets from above for call backs, prediction heat map and classification report\n",
    "<i>You will have provide a label list for this specific dataset inorder for them to run, you are to make the required changes yourself</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We hope all of you are working on your projects and <a href=\"https://ibb.co/dcpf4vS\"> Kudos for completing the assingnment</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
